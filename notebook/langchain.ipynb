{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# langchain guide\n",
    "> 20230414"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "基于Langchain构建LLM应用\n",
    "https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/unstructured_file.html\n",
    "https://eyurtsev.github.io/kor/   extract structured data from text using large language models (LLMs) 🧩.\n",
    "\"\"\"\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from kor import create_extraction_chain, Object, Text, extract_from_documents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# 初始化模型服务\n",
    "key = \"xxxxx\"\n",
    "os.environ['OPENAI_API_KEY'] = key\n",
    "\n",
    "openai_llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    max_tokens=2000,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    top_p=1.0,\n",
    "    request_timeout=120,\n",
    "    max_retries=10\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# 定义schema\n",
    "schema = Object(\n",
    "  id=\"info\",\n",
    "  description=(\n",
    "      \"关于临时公告-停牌信息的抽取--包含了公司名称，停牌日期，停牌天数。\"\n",
    "      \" music by a particular artist.\"\n",
    "  ),\n",
    "  attributes=[\n",
    "      Text(\n",
    "          id=\"company_name\",\n",
    "          description=\"公告涉及的公司名称\",\n",
    "          examples=[],\n",
    "          many=True,\n",
    "      ),\n",
    "      Text(\n",
    "          id=\"date\",\n",
    "          description=\"公告涉及的停牌日期\",\n",
    "          examples=[],\n",
    "          many=True,\n",
    "      ),\n",
    "      Text(\n",
    "          id=\"day\",\n",
    "          description=\"公告涉及的停牌天数\",\n",
    "          examples=[(\"股票将于2021年4月27日停牌1天\", \"1天\")],\n",
    "          many=True,\n",
    "      ),\n",
    "    ],\n",
    "  many=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/openai.py:Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI.\n",
      "WARNING:/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/openai.py:Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI.\n",
      "WARNING:/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/openai.py:Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI.\n",
      "WARNING:/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/openai.py:Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised APIConnectionError: Error communicating with OpenAI.\n",
      "WARNING:/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/openai.py:Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised APIConnectionError: Error communicating with OpenAI.\n",
      "WARNING:/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/openai.py:Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised APIConnectionError: Error communicating with OpenAI.\n",
      "WARNING:/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/openai.py:Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised APIConnectionError: Error communicating with OpenAI.\n",
      "WARNING:/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/openai.py:Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised APIConnectionError: Error communicating with OpenAI.\n",
      "WARNING:/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/openai.py:Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised APIConnectionError: Error communicating with OpenAI.\n",
      "ERROR:asyncio:Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x11e1b87c0>\n",
      "/Users/memect/.pyenv/versions/3.8.16/lib/python3.8/ast.py:47: RuntimeWarning: coroutine 'extract_from_documents' was never awaited\n",
      "  return compile(source, filename, mode, flags,\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "Error communicating with OpenAI",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectionResetError\u001B[0m                      Traceback (most recent call last)",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/aiohttp/connector.py:980\u001B[0m, in \u001B[0;36mTCPConnector._wrap_create_connection\u001B[0;34m(self, req, timeout, client_error, *args, **kwargs)\u001B[0m\n\u001B[1;32m    979\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m ceil_timeout(timeout\u001B[38;5;241m.\u001B[39msock_connect):\n\u001B[0;32m--> 980\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loop\u001B[38;5;241m.\u001B[39mcreate_connection(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[return-value]  # noqa\u001B[39;00m\n\u001B[1;32m    981\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m cert_errors \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/asyncio/base_events.py:1050\u001B[0m, in \u001B[0;36mBaseEventLoop.create_connection\u001B[0;34m(self, protocol_factory, host, port, ssl, family, proto, flags, sock, local_addr, server_hostname, ssl_handshake_timeout, happy_eyeballs_delay, interleave)\u001B[0m\n\u001B[1;32m   1047\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1048\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mA Stream Socket was expected, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msock\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m-> 1050\u001B[0m transport, protocol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_transport(\n\u001B[1;32m   1051\u001B[0m     sock, protocol_factory, ssl, server_hostname,\n\u001B[1;32m   1052\u001B[0m     ssl_handshake_timeout\u001B[38;5;241m=\u001B[39mssl_handshake_timeout)\n\u001B[1;32m   1053\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_debug:\n\u001B[1;32m   1054\u001B[0m     \u001B[38;5;66;03m# Get the socket from the transport because SSL transport closes\u001B[39;00m\n\u001B[1;32m   1055\u001B[0m     \u001B[38;5;66;03m# the old socket and creates a new SSL socket\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/asyncio/base_events.py:1080\u001B[0m, in \u001B[0;36mBaseEventLoop._create_connection_transport\u001B[0;34m(self, sock, protocol_factory, ssl, server_hostname, server_side, ssl_handshake_timeout)\u001B[0m\n\u001B[1;32m   1079\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1080\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m waiter\n\u001B[1;32m   1081\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "\u001B[0;31mConnectionResetError\u001B[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mClientConnectorError\u001B[0m                      Traceback (most recent call last)",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/openai/api_requestor.py:588\u001B[0m, in \u001B[0;36mAPIRequestor.arequest_raw\u001B[0;34m(self, method, url, session, params, supplied_headers, files, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    587\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 588\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m session\u001B[38;5;241m.\u001B[39mrequest(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrequest_kwargs)\n\u001B[1;32m    589\u001B[0m     util\u001B[38;5;241m.\u001B[39mlog_info(\n\u001B[1;32m    590\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOpenAI API response\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    591\u001B[0m         path\u001B[38;5;241m=\u001B[39mabs_url,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    594\u001B[0m         request_id\u001B[38;5;241m=\u001B[39mresult\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX-Request-Id\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    595\u001B[0m     )\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/aiohttp/client.py:536\u001B[0m, in \u001B[0;36mClientSession._request\u001B[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, proxy_headers, trace_request_ctx, read_bufsize)\u001B[0m\n\u001B[1;32m    535\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connector \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 536\u001B[0m         conn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connector\u001B[38;5;241m.\u001B[39mconnect(\n\u001B[1;32m    537\u001B[0m             req, traces\u001B[38;5;241m=\u001B[39mtraces, timeout\u001B[38;5;241m=\u001B[39mreal_timeout\n\u001B[1;32m    538\u001B[0m         )\n\u001B[1;32m    539\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mTimeoutError \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/aiohttp/connector.py:540\u001B[0m, in \u001B[0;36mBaseConnector.connect\u001B[0;34m(self, req, traces, timeout)\u001B[0m\n\u001B[1;32m    539\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 540\u001B[0m     proto \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection(req, traces, timeout)\n\u001B[1;32m    541\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_closed:\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/aiohttp/connector.py:901\u001B[0m, in \u001B[0;36mTCPConnector._create_connection\u001B[0;34m(self, req, traces, timeout)\u001B[0m\n\u001B[1;32m    900\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 901\u001B[0m     _, proto \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_direct_connection(req, traces, timeout)\n\u001B[1;32m    903\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m proto\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/aiohttp/connector.py:1206\u001B[0m, in \u001B[0;36mTCPConnector._create_direct_connection\u001B[0;34m(self, req, traces, timeout, client_error)\u001B[0m\n\u001B[1;32m   1205\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m last_exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1206\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m last_exc\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/aiohttp/connector.py:1175\u001B[0m, in \u001B[0;36mTCPConnector._create_direct_connection\u001B[0;34m(self, req, traces, timeout, client_error)\u001B[0m\n\u001B[1;32m   1174\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1175\u001B[0m     transp, proto \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_create_connection(\n\u001B[1;32m   1176\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_factory,\n\u001B[1;32m   1177\u001B[0m         host,\n\u001B[1;32m   1178\u001B[0m         port,\n\u001B[1;32m   1179\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m   1180\u001B[0m         ssl\u001B[38;5;241m=\u001B[39msslcontext,\n\u001B[1;32m   1181\u001B[0m         family\u001B[38;5;241m=\u001B[39mhinfo[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfamily\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   1182\u001B[0m         proto\u001B[38;5;241m=\u001B[39mhinfo[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproto\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   1183\u001B[0m         flags\u001B[38;5;241m=\u001B[39mhinfo[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflags\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   1184\u001B[0m         server_hostname\u001B[38;5;241m=\u001B[39mhinfo[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhostname\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m sslcontext \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1185\u001B[0m         local_addr\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_local_addr,\n\u001B[1;32m   1186\u001B[0m         req\u001B[38;5;241m=\u001B[39mreq,\n\u001B[1;32m   1187\u001B[0m         client_error\u001B[38;5;241m=\u001B[39mclient_error,\n\u001B[1;32m   1188\u001B[0m     )\n\u001B[1;32m   1189\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ClientConnectorError \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/aiohttp/connector.py:988\u001B[0m, in \u001B[0;36mTCPConnector._wrap_create_connection\u001B[0;34m(self, req, timeout, client_error, *args, **kwargs)\u001B[0m\n\u001B[1;32m    987\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m--> 988\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m client_error(req\u001B[38;5;241m.\u001B[39mconnection_key, exc) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n",
      "\u001B[0;31mClientConnectorError\u001B[0m: Cannot connect to host api.openai.com:443 ssl:default [None]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mAPIConnectionError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m extraction_chain \u001B[38;5;241m=\u001B[39m create_extraction_chain(openai_llm, schema, encoder_or_encoder_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStart extract from doc……\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 8\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m extract_from_documents(\n\u001B[1;32m      9\u001B[0m         chain\u001B[38;5;241m=\u001B[39mextraction_chain,\n\u001B[1;32m     10\u001B[0m         documents\u001B[38;5;241m=\u001B[39mdocs,\n\u001B[1;32m     11\u001B[0m         use_uid\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     12\u001B[0m         max_concurrency\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     13\u001B[0m     )\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m result:\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/kor/extraction/api.py:172\u001B[0m, in \u001B[0;36mextract_from_documents\u001B[0;34m(chain, documents, max_concurrency, use_uid, extraction_uid_function, return_exceptions)\u001B[0m\n\u001B[1;32m    160\u001B[0m     extraction_uid \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    161\u001B[0m         extraction_uid_function(doc) \u001B[38;5;28;01mif\u001B[39;00m extraction_uid_function \u001B[38;5;28;01melse\u001B[39;00m source_uid\n\u001B[1;32m    162\u001B[0m     )\n\u001B[1;32m    164\u001B[0m     tasks\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m    165\u001B[0m         asyncio\u001B[38;5;241m.\u001B[39mensure_future(\n\u001B[1;32m    166\u001B[0m             _extract_from_document_with_semaphore(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    169\u001B[0m         )\n\u001B[1;32m    170\u001B[0m     )\n\u001B[0;32m--> 172\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mgather(\u001B[38;5;241m*\u001B[39mtasks, return_exceptions\u001B[38;5;241m=\u001B[39mreturn_exceptions)\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m results\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/kor/extraction/api.py:28\u001B[0m, in \u001B[0;36m_extract_from_document_with_semaphore\u001B[0;34m(semaphore, chain, document, uid, source_uid)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Extract from document with a semaphore to limit concurrency.\"\"\"\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m semaphore:\n\u001B[1;32m     27\u001B[0m     extraction_result: Extraction \u001B[38;5;241m=\u001B[39m cast(\n\u001B[0;32m---> 28\u001B[0m         Extraction, \u001B[38;5;28;01mawait\u001B[39;00m chain\u001B[38;5;241m.\u001B[39mapredict_and_parse(text\u001B[38;5;241m=\u001B[39mdocument\u001B[38;5;241m.\u001B[39mpage_content)\n\u001B[1;32m     29\u001B[0m     )\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m     31\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muid\u001B[39m\u001B[38;5;124m\"\u001B[39m: uid,\n\u001B[1;32m     32\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_uid\u001B[39m\u001B[38;5;124m\"\u001B[39m: source_uid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     36\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merrors\u001B[39m\u001B[38;5;124m\"\u001B[39m: extraction_result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merrors\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     37\u001B[0m     }\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chains/llm.py:181\u001B[0m, in \u001B[0;36mLLMChain.apredict_and_parse\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapredict_and_parse\u001B[39m(\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any\n\u001B[1;32m    179\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[\u001B[38;5;28mstr\u001B[39m, List[\u001B[38;5;28mstr\u001B[39m], Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]]:\n\u001B[1;32m    180\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call apredict and then parse the results.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 181\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapredict(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprompt\u001B[38;5;241m.\u001B[39moutput_parser \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    183\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprompt\u001B[38;5;241m.\u001B[39moutput_parser\u001B[38;5;241m.\u001B[39mparse(result)\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chains/llm.py:167\u001B[0m, in \u001B[0;36mLLMChain.apredict\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    154\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001B[39;00m\n\u001B[1;32m    155\u001B[0m \n\u001B[1;32m    156\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001B[39;00m\n\u001B[1;32m    166\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 167\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39macall(kwargs))[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_key]\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chains/base.py:154\u001B[0m, in \u001B[0;36mChain.acall\u001B[0;34m(self, inputs, return_only_outputs)\u001B[0m\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    153\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose)\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mis_async:\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose)\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chains/base.py:148\u001B[0m, in \u001B[0;36mChain.acall\u001B[0;34m(self, inputs, return_only_outputs)\u001B[0m\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[1;32m    143\u001B[0m         {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m    144\u001B[0m         inputs,\n\u001B[1;32m    145\u001B[0m         verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[1;32m    146\u001B[0m     )\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 148\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_acall(inputs)\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mis_async:\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chains/llm.py:135\u001B[0m, in \u001B[0;36mLLMChain._acall\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_acall\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maapply([inputs]))[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chains/llm.py:123\u001B[0m, in \u001B[0;36mLLMChain.aapply\u001B[0;34m(self, input_list)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21maapply\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_list: List[Dict[\u001B[38;5;28mstr\u001B[39m, Any]]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]]:\n\u001B[1;32m    122\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 123\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magenerate(input_list)\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chains/llm.py:67\u001B[0m, in \u001B[0;36mLLMChain.agenerate\u001B[0;34m(self, input_list)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001B[39;00m\n\u001B[1;32m     66\u001B[0m prompts, stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maprep_prompts(input_list)\n\u001B[0;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39magenerate_prompt(prompts, stop)\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/base.py:106\u001B[0m, in \u001B[0;36mBaseChatModel.agenerate_prompt\u001B[0;34m(self, prompts, stop)\u001B[0m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    105\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose)\n\u001B[0;32m--> 106\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mis_async:\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mon_llm_end(output, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose)\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/base.py:100\u001B[0m, in \u001B[0;36mBaseChatModel.agenerate_prompt\u001B[0;34m(self, prompts, stop)\u001B[0m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[1;32m     97\u001B[0m         {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m}, prompt_strings, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose\n\u001B[1;32m     98\u001B[0m     )\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 100\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mis_async:\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/base.py:63\u001B[0m, in \u001B[0;36mBaseChatModel.agenerate\u001B[0;34m(self, messages, stop)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21magenerate\u001B[39m(\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m, messages: List[List[BaseMessage]], stop: Optional[List[\u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     61\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m     62\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Top Level call\"\"\"\u001B[39;00m\n\u001B[0;32m---> 63\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mgather(\n\u001B[1;32m     64\u001B[0m         \u001B[38;5;241m*\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_agenerate(m, stop\u001B[38;5;241m=\u001B[39mstop) \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m messages]\n\u001B[1;32m     65\u001B[0m     )\n\u001B[1;32m     66\u001B[0m     llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n\u001B[1;32m     67\u001B[0m     generations \u001B[38;5;241m=\u001B[39m [res\u001B[38;5;241m.\u001B[39mgenerations \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results]\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/openai.py:318\u001B[0m, in \u001B[0;36mChatOpenAI._agenerate\u001B[0;34m(self, messages, stop)\u001B[0m\n\u001B[1;32m    316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ChatResult(generations\u001B[38;5;241m=\u001B[39m[ChatGeneration(message\u001B[38;5;241m=\u001B[39mmessage)])\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 318\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m acompletion_with_retry(\n\u001B[1;32m    319\u001B[0m         \u001B[38;5;28mself\u001B[39m, messages\u001B[38;5;241m=\u001B[39mmessage_dicts, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[1;32m    320\u001B[0m     )\n\u001B[1;32m    321\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response)\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/openai.py:63\u001B[0m, in \u001B[0;36macompletion_with_retry\u001B[0;34m(llm, **kwargs)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;66;03m# Use OpenAI's async api https://github.com/openai/openai-python#async-api\u001B[39;00m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m llm\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39macreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m _completion_with_retry(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/tenacity/_asyncio.py:88\u001B[0m, in \u001B[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21masync_wrapped\u001B[39m(\u001B[38;5;241m*\u001B[39margs: t\u001B[38;5;241m.\u001B[39mAny, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: t\u001B[38;5;241m.\u001B[39mAny) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mAny:\n\u001B[0;32m---> 88\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/tenacity/_asyncio.py:47\u001B[0m, in \u001B[0;36mAsyncRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m retry_state \u001B[38;5;241m=\u001B[39m RetryCallState(retry_object\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, fn\u001B[38;5;241m=\u001B[39mfn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     do \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m     49\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/tenacity/__init__.py:325\u001B[0m, in \u001B[0;36mBaseRetrying.iter\u001B[0;34m(self, retry_state)\u001B[0m\n\u001B[1;32m    323\u001B[0m     retry_exc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry_error_cls(fut)\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreraise:\n\u001B[0;32m--> 325\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[43mretry_exc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m retry_exc \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfut\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexception\u001B[39;00m()\n\u001B[1;32m    328\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwait:\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/tenacity/__init__.py:158\u001B[0m, in \u001B[0;36mRetryError.reraise\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreraise\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mNoReturn:\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_attempt\u001B[38;5;241m.\u001B[39mfailed:\n\u001B[0;32m--> 158\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlast_attempt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    159\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/concurrent/futures/_base.py:437\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    436\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 437\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_condition\u001B[38;5;241m.\u001B[39mwait(timeout)\n\u001B[1;32m    441\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/concurrent/futures/_base.py:389\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 389\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    390\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    391\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    392\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/tenacity/_asyncio.py:50\u001B[0m, in \u001B[0;36mAsyncRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 50\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n\u001B[1;32m     52\u001B[0m         retry_state\u001B[38;5;241m.\u001B[39mset_exception(sys\u001B[38;5;241m.\u001B[39mexc_info())  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/langchain/chat_models/openai.py:61\u001B[0m, in \u001B[0;36macompletion_with_retry.<locals>._completion_with_retry\u001B[0;34m(**kwargs)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;66;03m# Use OpenAI's async api https://github.com/openai/openai-python#async-api\u001B[39;00m\n\u001B[0;32m---> 61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m llm\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39macreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/openai/api_resources/chat_completion.py:45\u001B[0m, in \u001B[0;36mChatCompletion.acreate\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 45\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39macreate(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TryAgain \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     47\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m start \u001B[38;5;241m+\u001B[39m timeout:\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py:217\u001B[0m, in \u001B[0;36mEngineAPIResource.acreate\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21macreate\u001B[39m(\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m    202\u001B[0m ):\n\u001B[1;32m    203\u001B[0m     (\n\u001B[1;32m    204\u001B[0m         deployment_id,\n\u001B[1;32m    205\u001B[0m         engine,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    215\u001B[0m         api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[1;32m    216\u001B[0m     )\n\u001B[0;32m--> 217\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m requestor\u001B[38;5;241m.\u001B[39marequest(\n\u001B[1;32m    218\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    219\u001B[0m         url,\n\u001B[1;32m    220\u001B[0m         params\u001B[38;5;241m=\u001B[39mparams,\n\u001B[1;32m    221\u001B[0m         headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    222\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    223\u001B[0m         request_id\u001B[38;5;241m=\u001B[39mrequest_id,\n\u001B[1;32m    224\u001B[0m         request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[1;32m    225\u001B[0m     )\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    228\u001B[0m         \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n\u001B[1;32m    229\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, OpenAIResponse)\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/openai/api_requestor.py:300\u001B[0m, in \u001B[0;36mAPIRequestor.arequest\u001B[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    298\u001B[0m session \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__aenter__\u001B[39m()\n\u001B[1;32m    299\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 300\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39marequest_raw(\n\u001B[1;32m    301\u001B[0m         method\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[1;32m    302\u001B[0m         url,\n\u001B[1;32m    303\u001B[0m         session,\n\u001B[1;32m    304\u001B[0m         params\u001B[38;5;241m=\u001B[39mparams,\n\u001B[1;32m    305\u001B[0m         supplied_headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    306\u001B[0m         files\u001B[38;5;241m=\u001B[39mfiles,\n\u001B[1;32m    307\u001B[0m         request_id\u001B[38;5;241m=\u001B[39mrequest_id,\n\u001B[1;32m    308\u001B[0m         request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[1;32m    309\u001B[0m     )\n\u001B[1;32m    310\u001B[0m     resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interpret_async_response(result, stream)\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n",
      "File \u001B[0;32m/Volumes/workbench/code/study/llm_app/chat_llm/venv/lib/python3.8/site-packages/openai/api_requestor.py:605\u001B[0m, in \u001B[0;36mAPIRequestor.arequest_raw\u001B[0;34m(self, method, url, session, params, supplied_headers, files, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    603\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mTimeout(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRequest timed out\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    604\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m aiohttp\u001B[38;5;241m.\u001B[39mClientError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 605\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mAPIConnectionError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError communicating with OpenAI\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mAPIConnectionError\u001B[0m: Error communicating with OpenAI"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "docs = [Document(page_content=\"提取一段文本\")]\n",
    "\n",
    "logging.info(\"Create extract chain\")\n",
    "extraction_chain = create_extraction_chain(openai_llm, schema, encoder_or_encoder_class='json')\n",
    "logging.info(\"Start extract from doc……\")\n",
    "result = await extract_from_documents(\n",
    "        chain=extraction_chain,\n",
    "        documents=docs,\n",
    "        use_uid=False,\n",
    "        max_concurrency=1\n",
    "    )\n",
    "\n",
    "\n",
    "for item in result:\n",
    "    if item['data']:\n",
    "        print(json.dumps(item['data'], ensure_ascii=False, indent=4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
