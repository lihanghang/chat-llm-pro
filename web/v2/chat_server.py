"""
åŸºäºgrad ioè¿›è¡Œåº”ç”¨æ„å»º
https://github.com/gradio-app/gradio/issues/3729
"""
import logging
import os
import pathlib
import pickle
import shutil
import sys

import gradio as gr
import openai


curPath = os.path.abspath(os.path.dirname(__file__))
rootPath = os.path.split(curPath)[0]
sys.path.insert(0, os.path.split(rootPath)[0])

from src.langchain.extract import parser_pdf, extract_doc, chat_mem_fin_llm
from web import host, port, model_name, openai_key, api_version, api_base, api_type
from data import example, prompt_text
from src.gpt import set_openai_key, GPT, Example
from src.utils.data_store import doc2embedding, save_embedding
from src.utils.doc import parser_doc, hashcode_with_file, get_file_ext_size

set_openai_key(openai_key)
#
# openai.api_base = api_base
# openai.api_version = api_version
# openai.api_type = api_type

model_type = 'MemFinLLM'
data_store_base_path = 'data/store'  # ç”Ÿæˆæ–‡ä»¶çˆ¶çº§ç›®å½•
store_origin_file_dir = None
mem_api_base = os.getenv('MEM_FIN_OPENAI_API')

gpt = GPT(engine=model_name, temperature=0.6, max_tokens=1024)


def init_store_dir(store_dir):
    """
    åˆå§‹åŒ–appæ—¶ åˆ é™¤æ•°æ®å­˜å‚¨ç›®å½•
    Returns: None

    """
    for root, dirs, files in os.walk(store_dir):
        for item in dirs:
            dir_path = os.path.join(root, item)
            shutil.rmtree(dir_path)
    logging.info("Clean store data.")


def get_embedding(query):
    embedding = openai.Embedding.create(engine="text-embedding-ada-002", input=query)
    return query, embedding.data[0].embedding, embedding.usage.total_tokens


def process_upload_file(file_tmp_path):
    """
    Receive upload file and process.
    Args:
        file_tmp_path:

    Returns:str  file_name

    """
    global store_origin_file_dir
    file_name_path = file_tmp_path.name
    doc_name_with_ext = file_name_path.split('/')[-1]  # ä¸Šä¼ æ–‡ä»¶çš„åç§°
    ext, file_size = get_file_ext_size(file_name_path)

    if str.lower(ext) not in ['.pdf', '.txt', '.docx', '.doc'] or file_size / 1024 / 1024 > 2:
        raise gr.Error(f"{ext} è¯·ç¡®è®¤æ–‡ä»¶æ ¼å¼å’Œå¤§å° {file_size / 1024 / 1024}M")

    file_hashcode = hashcode_with_file(file_name_path)
    store_origin_file_dir = f'{data_store_base_path}/{file_hashcode}'
    logging.info(store_origin_file_dir)

    if pathlib.Path(store_origin_file_dir).exists():  # å­˜åœ¨è¡¨ç¤ºæ–‡ä»¶å·²ç»å·²ç»ä¸Šä¼ è¿‡ï¼Œä¸åœ¨ç»§ç»­åç»­é€»è¾‘
        logging.info("upload file exists.")
    else:
        pathlib.Path(store_origin_file_dir).mkdir(parents=True, exist_ok=True)
        copy_upload_file = f'{store_origin_file_dir}/{doc_name_with_ext}'
        shutil.copyfile(file_name_path, copy_upload_file)
        output_text_file = parser_doc(copy_upload_file, store_origin_file_dir)  # ç»Ÿä¸€è§£æè¾“å‡ºä¸º.txt
        embedding_with_index: dict = doc2embedding(output_text_file)  # è½¬åŒ–ä¸ºè¯å‘é‡
        save_embedding(embedding_with_index, f'{store_origin_file_dir}/embedding.pickle')  # å­˜å‚¨è‡³æœ¬åœ°

    return f'{doc_name_with_ext}é¢„å¤„ç†å®Œæˆã€‚è¿™ç¯‡æ–‡æ¡£ä¸»è¦è®²äº†ä»¥ä¸‹å†…å®¹ã€‚'


def chat_doc(query, model_type, task_type='é—®ç­”'):
    import numpy as np
    # Load knowledge from store
    try:
        if not store_origin_file_dir:
            logging.warning("Not found doc vector file.")
            return "æ— docä¿¡æ¯ï¼Œè€ƒè™‘ä¸Šä¼ ä¸€ä»½æ–‡æ¡£åå†æé—®ã€‚"
        file = open(f'{store_origin_file_dir}/embedding.pickle', 'rb')
        emb_data = pickle.load(file)
        index, data = emb_data['index'], emb_data['embedding']
        logging.info("Success load doc vector file.")

        text, emb, query_token_num = get_embedding(query)  # compute query embedding
        logging.info(f"query token num:{query_token_num}")
        _, text_index = index.search(np.array([emb]), k=10)  # æ ¹æ®ç´¢å¼•ä»ä¸Šä¼ æ–‡æ¡£ä¸­æœç´¢ç›¸è¿‘çš„å†…å®¹
        context = []
        for i in list(text_index[0]):
            context.extend(data[i:i + 6])
        lens = [len(text) for text in context]
        logging.debug(f"åŒ¹é…åˆ°çš„æ–‡æœ¬é•¿åº¦å¤§å°ï¼š{lens}")
        maximum = 3000
        for index, l in enumerate(lens):
            maximum -= l
            if maximum < 0:
                context = context[:index + 1]
                logging.warning("è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œæˆªæ–­åˆ°å‰", index + 1, "ä¸ªç‰‡æ®µ")
                break

        text = "".join(text for _, text in enumerate(context))
        logging.info(f'Load model {model_type}')
        if model_type == 'gpt35':
            ret, tokens_num = gpt.get_top_reply(query, task_type, text)  # è¯·æ±‚LLM
            logging.debug(f'Context:{text}\nOutput:{ret}')
            logging.info(f"æœ¬è½®å¯¹è¯æ¶ˆè€—tokens:{tokens_num}")
            return f'{model_type}\n{ret}'
        else:
            ret = chat_mem_fin_llm(mem_api_base, text, task_type)
            logging.debug(f'Context:{text}\nOutput:{ret}')
            return f'ã€{model_type}ã€‘\n{ret}'
    except Exception as e:
        logging.error(e)


def add_examples(issue, reply):
    """Generate QA example"""
    return gpt.add_example(Example(issue, reply))


def del_all_examples():
    [gpt.delete_example(ex_id) for ex_id, _ in gpt.get_all_examples().items()]
    return gpt.get_all_examples()


def task_with_chat(input_txt, task, model_type):
    """
    å¯¹è¯å¼ä»»åŠ¡
    Returns: response

    """
    logging.info(f'Load model name:{model_type}')
    try:
        if model_type == 'gpt35':
            response, token_num = gpt.get_top_reply(input_txt, task)
            logging.info(f"text len:{len(input_txt)}. Consumer token num:{token_num}")
            return response
        elif model_type == 'all':
            mem_response = chat_mem_fin_llm(mem_api_base, input_txt, task)
            gpt_response, token_num = gpt.get_top_reply(input_txt, task)
            return f'ã€MemectFinLLMã€‘\n{mem_response} \n\nã€gptã€‘\n{gpt_response}'

        else:
            response = chat_mem_fin_llm(mem_api_base, input_txt, task)
            logging.info(response)
            return response
    except Exception as e:
        gr.Error(e)


def extract_chain(file_path, schema, model_type):
    docs = parser_pdf(file_path)
    ret = extract_doc(schema, model_type, docs)
    return ret


with gr.Blocks(css="footer {visibility: hidden}", title='ChatLLM is all you need') as demo:
    gr.Markdown(f"<h1 style='text-align: center;'>å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ä½“éªŒ</h1>")
    gr.Markdown(f'> Model by {model_type}. Contact us via https://www.memect.cn/ .')

    with gr.Tab("åœºæ™¯é—®ç­”"):
        with gr.Row():
            with gr.Column():
                input_text = gr.Textbox(label="æˆ‘è¦æé—®", value="ä»‹ç»ä¸‹è‡ªå·±ï¼Ÿ")
                model_type = gr.Dropdown(choices=["MemeFinLLMï¼ˆæ–‡å› é‡‘èå¤§æ¨¡å‹ï¼‰", "gpt35", "all"], value='æ–‡å› é‡‘èå¤§æ¨¡å‹', label='é€‰æ‹©æ¨¡å‹')
                task_type = gr.Radio(choices=list(prompt_text.keys()),
                                     label="åœºæ™¯ç±»å‹", value='é—®ç­”')
                submit = gr.Button("é—®ä¸€ä¸‹")
            with gr.Column():
                output_ret = gr.Text(label='è¾“å‡º')
        submit.click(fn=task_with_chat, inputs=[input_text, task_type, model_type], outputs=output_ret)
        gr.Examples(example, [input_text, task_type])

    with gr.Tab("MemChatDocï¼ˆæ–‡æ¡£é—®ç­”ï¼‰"):  # æ ¹æ®æ–‡æ¡£è¿›è¡Œæé—®
        def add_file(history, doc):
            history = history + [(process_upload_file(doc), None)]
            return history


        def add_text(history, inp):
            history = history + [(inp, None)]
            return history, ""


        def bot(history, model_type):
            history[-1][1] = chat_doc(query=history[-1][0], model_type=model_type)
            return history


        chatbot = gr.Chatbot([("Welcome MemChatDoc. Please upload doc.", None)], show_label=False,
                             elem_id='chatbot').style(height="100%")
        model_type = gr.Dropdown(choices=["MemeFinLLMï¼ˆæ–‡å› é‡‘èå¤§æ¨¡å‹ï¼‰", "gpt35"], value='æ–‡å› é‡‘èå¤§æ¨¡å‹',
                                 label='é€‰æ‹©æ¨¡å‹')
        state = gr.State([])
        with gr.Row():
            with gr.Column(scale=0.85):
                txt = gr.Textbox(
                    show_label=False,
                    placeholder='Enter question and press enter, or upload an file'
                ).style(container=False)

            with gr.Column(scale=0.15, min_width=0):
                btn = gr.UploadButton(label="ğŸ“ä¸Šä¼ æ–‡æ¡£", file_types=['file'])

        txt.submit(add_text, inputs=[chatbot, txt], outputs=[chatbot, txt], queue=False).then(bot, [chatbot, model_type], chatbot)
        btn.upload(add_file, inputs=[chatbot, btn], outputs=[chatbot]).then(bot, [chatbot, model_type], chatbot)

        clear = gr.Button("Clear")
        clear.click(lambda: None, None, chatbot, queue=False)

    # with gr.Tab("docExtractorï¼ˆæ–‡æ¡£æŠ½å–ï¼‰å¼€å‘ä¸­"):
    #     file_output = gr.File(label="ä¸Šä¼ æ–‡æ¡£")
    #
    #     choice_model = gr.Dropdown(choices=["MemectLLM", "GPT35"], value="MemectLLM", label="é€‰æ‹©æ¨¡å‹")
    #     schema = gr.Code(language='python', label="å®šä¹‰æŠ½å–è¦ç´ ")
    #     extract_result = gr.Textbox(label='result')
    #
    #     doc_btn = gr.Button("extract")
    #     doc_btn.click(extract_chain, inputs=[file_output, schema, choice_model], outputs=extract_result)

    with gr.Tab("å¢åŠ æ¨¡å‹çŸ¥è¯†"):
        with gr.Column():  # åˆ—æ’åˆ—
            question = gr.Textbox(label="question", value="æ–‡å› äº’è”æ˜¯åšä»€ä¹ˆçš„?")
            answer = gr.Textbox(label="answer",
                                value="æ–‡å› äº’è”æ˜¯å›½å†…é¢†å…ˆçš„é‡‘èè®¤çŸ¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆçš„æä¾›å•†ï¼Œåœ¨çŸ¥è¯†å›¾è°±æŠ€æœ¯ã€è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ã€é‡‘èçŸ¥è¯†å»ºæ¨¡ç­‰æ–¹é¢æœ‰æ·±åšç§¯æ·€ã€‚")
            result = gr.Textbox(label='result')
        submit_example = gr.Button("submit_example")
        clean_example = gr.Button("clean_example")
        submit_example.click(fn=add_examples, inputs=[question, answer], outputs=result)
        clean_example.click(del_all_examples, inputs=[], outputs=result)

init_store_dir(data_store_base_path)
demo.launch(server_name=host, server_port=int(port), share=True)
